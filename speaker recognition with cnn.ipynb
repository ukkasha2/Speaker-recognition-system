{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40db30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, models\n",
    "import sounddevice as sd\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af8888a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction():\n",
    "    # Define the path to the main folder containing the voice samples\n",
    "    folder_path = \"voice_samples\"\n",
    "\n",
    "    # Define the desired duration in seconds\n",
    "    desired_duration = 5\n",
    "\n",
    "    # Initialize empty lists to store the features and corresponding labels\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Iterate over the subfolders and files within the main folder\n",
    "    for subfolder in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            label = subfolder  # Assume the subfolder name is the label\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                file_path = os.path.join(subfolder_path, filename)\n",
    "                if filename.endswith(\".wav\"):\n",
    "                    # Load the audio file using librosa\n",
    "                    audio, sr = librosa.load(file_path)\n",
    "\n",
    "                    # Calculate the number of samples for the desired duration\n",
    "                    desired_samples = int(desired_duration * sr)\n",
    "\n",
    "                    # Trim the audio to the desired duration\n",
    "                    if len(audio) > desired_samples:\n",
    "                        audio = audio[:desired_samples]\n",
    "                    else:\n",
    "                        # If the audio is shorter than the desired duration, pad it with zeros\n",
    "                        audio = np.pad(audio, (0, desired_samples - len(audio)), \"constant\")\n",
    "\n",
    "                    # Extract the MFCC features\n",
    "                    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=50)\n",
    "\n",
    "                    # Append the MFCC features and label to the main lists\n",
    "                    all_features.append(mfcc)\n",
    "                    all_labels.append(label)\n",
    "\n",
    "    # Convert the feature and label lists to numpy arrays\n",
    "    all_features = np.array(all_features)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Save the features and labels as .npy files\n",
    "    np.save(\"features.npy\", all_features)\n",
    "    np.save(\"labels.npy\", all_labels)\n",
    "    print('features and label saved succesfully')\n",
    "\n",
    "    # Print the shape of the extracted features and labels\n",
    "    print(\"Features shape:\", all_features.shape)\n",
    "    print(\"Labels shape:\", all_labels.shape)\n",
    "    \n",
    "def train_model(x_train, x_test):\n",
    "    # Reshape the input features\n",
    "    input_shape = x_train.shape[1:]\n",
    "    x_train = x_train.reshape((*x_train.shape, 1))\n",
    "    x_test = x_test.reshape((*x_test.shape, 1))\n",
    "\n",
    "    # Define the CNN model architecture\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Reshape((50, 216, 1), input_shape=input_shape))\n",
    "    model.add(layers.Conv2D(32, kernel_size = (3, 3), strides = (1, 1), activation='relu', input_shape = input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(3 ,activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"newest_trained_model.h5\")\n",
    "    print('model trained succesfully')\n",
    "    \n",
    "def calculate_similarity(mfcc1, mfcc2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two sets of MFCC features.\n",
    "    \n",
    "    Parameters:\n",
    "        mfcc1 (numpy.ndarray): MFCC features of the first speaker.\n",
    "        mfcc2 (numpy.ndarray): MFCC features of the second speaker.\n",
    "    \n",
    "    Returns:\n",
    "        float: Cosine similarity score.\n",
    "    \"\"\"\n",
    "    # Reshape MFCC arrays to (n_frames, n_mfcc)\n",
    "    mfcc1 = mfcc1.reshape(-1, mfcc1.shape[-1])\n",
    "    mfcc2 = mfcc2.reshape(-1, mfcc2.shape[-1])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(mfcc1, mfcc2)\n",
    "    \n",
    "    # Take the average similarity score\n",
    "    similarity_score = np.mean(similarity_matrix)\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "def augment_audio(audio, sr):\n",
    "    # Randomly choose augmentation types and parameters\n",
    "    augmentation_types = ['add_noise', 'change_pitch', 'change_speed']\n",
    "    chosen_augmentations = random.sample(augmentation_types, k=random.randint(1, len(augmentation_types)))\n",
    "\n",
    "    augmented_audio = audio.copy()\n",
    "\n",
    "    for augmentation in chosen_augmentations:\n",
    "        if augmentation == 'add_noise':\n",
    "            # Add random noise to the audio\n",
    "            noise_level = random.uniform(0.001, 0.01)  # Adjust noise level as needed\n",
    "            augmented_audio = librosa.effects.preemphasis(augmented_audio, coef=noise_level)\n",
    "\n",
    "        elif augmentation == 'change_pitch':\n",
    "            # Randomly change the pitch within a certain range\n",
    "            pitch_shift = random.uniform(-2, 2)  # Adjust pitch shift range as needed\n",
    "            augmented_audio = librosa.effects.pitch_shift(augmented_audio, sr=sr, n_steps=pitch_shift)\n",
    "\n",
    "        elif augmentation == 'change_speed':\n",
    "            # Randomly change the playback speed within a certain range\n",
    "            speed_factor = random.uniform(0.9, 1.1)  # Adjust speed factor range as needed\n",
    "            augmented_audio = librosa.effects.time_stretch(augmented_audio, rate=speed_factor)\n",
    "\n",
    "    return augmented_audio\n",
    "\n",
    "def apply_snr_filter(audio_data, threshold = 20):\n",
    "    preemphasized_audio = librosa.effects.preemphasis(audio_data)\n",
    "    spectogram = librosa.feature.melspectrogram(y=preemphasized_audio)\n",
    "    s_centroid = librosa.feature.spectral_centroid(S=spectogram)[0, :]\n",
    "    \n",
    "    min_length = min(len(audio_data), len(s_centroid))\n",
    "    audio_data = audio_data[:min_length]\n",
    "    s_centroid = s_centroid[:min_length]\n",
    "    \n",
    "    filtered_audio = audio_data[s_centroid > threshold]\n",
    "    return filtered_audio\n",
    "\n",
    "def extract_mfcc_features_from_file(file_path, desired_duration=5):\n",
    "    try:\n",
    "        # Load the audio file using librosa\n",
    "        audio, sr = librosa.load(file_path)\n",
    "    \n",
    "        # Apply data augmentation to the audio (you can remove this line if not needed)\n",
    "        #audio = augment_audio(audio, sr)\n",
    "        #audio = apply_snr_filter(audio_aug)\n",
    "        \n",
    "    \n",
    "        # Calculate the number of samples for the desired duration\n",
    "        desired_samples = int(desired_duration * sr)\n",
    "    \n",
    "        # Trim or pad the audio to the desired duration\n",
    "        if len(audio) > desired_samples:\n",
    "            audio = audio[:desired_samples]\n",
    "        else:\n",
    "            audio = np.pad(audio, (0, desired_samples - len(audio)), \"constant\")\n",
    "    \n",
    "        # Extract the MFCC features\n",
    "        user_mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=50)\n",
    "        \n",
    "        return user_mfcc\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None\n",
    "    \n",
    "def record_audio_and_save_it(duration=5, sample_rate=44100):\n",
    "    for i in range(3,-1,-1):\n",
    "        print(f'Recording in {i}', end='\\r')\n",
    "        time.sleep(1)\n",
    "    print('Listening...')\n",
    "    recorded_audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype=np.float32)\n",
    "    sd.wait()\n",
    "    print('Recording complete.')\n",
    "\n",
    "    # Save the recorded audio as a .wav file\n",
    "    file_path = 'recorded_audio.wav'\n",
    "    wavfile.write(file_path, sample_rate, recorded_audio)\n",
    "    return extract_mfcc_features_from_file(file_path)\n",
    "\n",
    "\n",
    "def make_predcitions(user_mfcc):\n",
    "    user_mfcc = user_mfcc.reshape((1, 50, 216, 1))\n",
    "    # Load the pre-trained model\n",
    "    model = models.load_model(r'C:\\Users\\UKKASHA\\Anaconda3\\man\\newest_trained_model.h5')\n",
    "\n",
    "    # Define confidence and similarity thresholds\n",
    "    confidence_threshold = 99  # Adjust this threshold as needed\n",
    "    similarity_threshold = 75  # Adjust this threshold as needed\n",
    "\n",
    "    # Perform the prediction for the user's voice\n",
    "    predictions = model.predict(user_mfcc)\n",
    "    predicted_index = np.argmax(predictions[0])\n",
    "    confidence = np.round((predictions[0][predicted_index] * 100), 2)\n",
    "    predicted_label = label_encoder.classes_[predicted_index]\n",
    "    #user_name = input('Enter your name: ')\n",
    "    user_name = predicted_label\n",
    "    \n",
    "    if user_name in ''.join(label_encoder.classes_):\n",
    "#         print(predicted_label)\n",
    "#         print(confidence)\n",
    "        if user_name in predicted_label:\n",
    "            if confidence > confidence_threshold:\n",
    "                return f'{predicted_label} - ({confidence}%)'\n",
    "            else:\n",
    "                return f'predicted: Unknown ({confidence}%)'\n",
    "# #                 print('Access Granted!')\n",
    "# #                 print(f\"Predicted Speaker: {predicted_label}\")\n",
    "# #                 print(f\"Confidence: {confidence}\")\n",
    "#             else:\n",
    "# #                 print('Access Denied!')\n",
    "# #                 print(\"Can't tell confidently if you are who you are trying to claim\")\n",
    "#         else:\n",
    "# #             print('You are not who you are trying to claim')\n",
    "#     else:\n",
    "# #         print('Unknown user!')\n",
    "\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "def say(text):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.save('ouput.mp3')\n",
    "    os.system('start ouput.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dc3c958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000029029021310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyttsx3/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000029029021520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyttsx3/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000290290216D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyttsx3/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000029029021880>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyttsx3/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000029029021A30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyttsx3/\n",
      "ERROR: Could not find a version that satisfies the requirement pyttsx3 (from versions: none)\n",
      "ERROR: No matching distribution found for pyttsx3\n"
     ]
    }
   ],
   "source": [
    "pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecda509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted features and labels\n",
    "features = np.load(r\"C:\\Users\\UKKASHA\\Anaconda3\\man\\features.npy\")\n",
    "labels = np.load(r\"C:\\Users\\UKKASHA\\Anaconda3\\man\\labels.npy\")\n",
    "    \n",
    "# Perform label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels_encoded,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47a95e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_mfcc_features_from_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUKKASHA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrecorded_audio.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m user_mfcc \u001b[38;5;241m=\u001b[39m \u001b[43mextract_mfcc_features_from_file\u001b[49m(path)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(make_predcitions(user_mfcc))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_mfcc_features_from_file' is not defined"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\UKKASHA\\Downloads\\recorded_audio.wav'\n",
    "user_mfcc = extract_mfcc_features_from_file(path)\n",
    "print(make_predcitions(user_mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7c93e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 933ms/step\n",
      "actual: Albani zaria, predicted: albani zaria (100.0%)\n",
      "1/1 [==============================] - 1s 906ms/step\n",
      "actual: Albani zaria, predicted: albani zaria (100.0%)\n",
      "1/1 [==============================] - 1s 812ms/step\n",
      "actual: Albani zaria, predicted: albani zaria (100.0%)\n",
      "1/1 [==============================] - 1s 812ms/step\n",
      "actual: Okasha Kameny, predicted: okasha kameny (99.99%)\n",
      "1/1 [==============================] - 1s 844ms/step\n",
      "actual: Okasha Kameny, predicted: okasha kameny (100.0%)\n",
      "1/1 [==============================] - 1s 810ms/step\n",
      "actual: Okasha Kameny, predicted: okasha kameny (100.0%)\n",
      "1/1 [==============================] - 1s 812ms/step\n",
      "actual: Okasha Kameny, predicted: Unknown (93.5%)\n",
      "1/1 [==============================] - 1s 866ms/step\n",
      "actual: Okasha Kameny, predicted: okasha kameny (100.0%)\n",
      "1/1 [==============================] - 1s 829ms/step\n",
      "actual: Okasha Kameny, predicted: okasha kameny (100.0%)\n",
      "1/1 [==============================] - 1s 812ms/step\n",
      "actual: Okasha Kameny, predicted: Unknown (98.4%)\n",
      "1/1 [==============================] - 1s 812ms/step\n",
      "actual: Okasha Kameny, predicted: okasha kameny (100.0%)\n",
      "1/1 [==============================] - 1s 797ms/step\n",
      "actual: Okasha Kameny, predicted: omar sulaiman (100.0%)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "actual: Omar Sulaiman, predicted: omar sulaiman (100.0%)\n",
      "1/1 [==============================] - 1s 813ms/step\n",
      "actual: Omar Sulaiman, predicted: omar sulaiman (100.0%)\n",
      "1/1 [==============================] - 1s 812ms/step\n",
      "actual: Omar Sulaiman, predicted: omar sulaiman (100.0%)\n"
     ]
    }
   ],
   "source": [
    "actual = ['Albani zaria','Albani zaria','Albani zaria',\n",
    "          'Okasha Kameny', 'Okasha Kameny', 'Okasha Kameny', 'Okasha Kameny',\n",
    "          'Okasha Kameny', 'Okasha Kameny', 'Okasha Kameny', 'Okasha Kameny',\n",
    "          'Okasha Kameny','Omar Sulaiman','Omar Sulaiman','Omar Sulaiman',\n",
    "          'Omar Sulaiman','Omar Sulaiman']\n",
    "for i in range(15):\n",
    "    path = fr'C:\\Users\\UKKASHA\\Anaconda3\\man\\test data\\test_{i+1}.wav'\n",
    "    user_mfcc = extract_mfcc_features_from_file(path)\n",
    "    print(make_predcitions(user_mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "audio, sr = librosa.load('recorded_audio.wav')\n",
    "spectogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(spectogram, ref=np.max), y_axis='mel', x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09375cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "audio, sr = librosa.load('recorded_audioo.wav')\n",
    "spectogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(spectogram, ref=np.max), y_axis='mel', x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c502ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "audio, sr = librosa.load(r'C:\\Users\\UKKASHA\\Anaconda3\\man\\test data\\test_1.wav')\n",
    "spectogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(spectogram, ref=np.max), y_axis='mel', x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79709ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "audio, sr = librosa.load(r'C:\\Users\\UKKASHA\\Anaconda3\\man\\test data\\test_5.wav')\n",
    "spectogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(spectogram, ref=np.max), y_axis='mel', x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a85a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "audio, sr = librosa.load(r'C:\\Users\\UKKASHA\\Anaconda3\\man\\test data\\test_15.wav')\n",
    "spectogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(spectogram, ref=np.max), y_axis='mel', x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5facf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(r'C:\\Users\\UKKASHA\\Anaconda3\\man\\test data\\test_1.wav')\n",
    "librosa.display.waveshow(y=audio, sr=sr)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(r'C:\\Users\\UKKASHA\\Anaconda3\\man\\test data\\test_5.wav')\n",
    "librosa.display.waveshow(y=audio, sr=sr)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_test[80])\n",
    "# print(np.argmax(y_pred[80]))\n",
    "np.argmax(y_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435dadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.load_model(r'C:\\Users\\UKKASHA\\Anaconda3\\man\\newest_trained_model.h5')\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149abb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred_classes, target_names=list(set(labels)), output_dict=True)\n",
    "classification_report_values = list(report.values())\n",
    "classification_report_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "data = np.array([[item['precision'], item['recall'], item['f1-score']] for item in classification_report_values if isinstance(item, dict)])\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(data, annot=True, fmt=\".3f\", cmap=\"Blues\", xticklabels=['Precision', 'Recall', 'F1-Score'], yticklabels=list(set(labels))\n",
    "plt.title('Classification Metrics by Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "classification_report_values = [\n",
    "    {'precision': 1.0, 'recall': 0.9965811965811966, 'f1-score': 0.9982876712328768, 'support': 585},\n",
    "    {'precision': 0.9950248756218906, 'recall': 0.9983361064891847, 'f1-score': 0.9966777408637874, 'support': 601},\n",
    "    {'precision': 0.998371335504886, 'recall': 0.998371335504886, 'f1-score': 0.998371335504886, 'support': 614},\n",
    "    0.9977777777777778,\n",
    "    {'precision': 0.9977987370422587, 'recall': 0.997762879525089, 'f1-score': 0.9977789158671833, 'support': 1800}\n",
    "]\n",
    "\n",
    "class_names = ['Class 1', 'Class 2', 'Class 3']\n",
    "\n",
    "# Extract values for plotting, excluding float values (support for all classes combined)\n",
    "data = np.array([[item['precision'], item['recall'], item['f1-score']] for item in classification_report_values if isinstance(item, dict)])\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(data, annot=True, fmt=\".3f\", cmap=\"Blues\", xticklabels=['Precision', 'Recall', 'F1-Score'], yticklabels=list(set(labels)))\n",
    "plt.title('Classification Report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb80aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
